{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Naveen\\Desktop\\GenAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\Naveen\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(repo_id = repo_id)\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    query_instruction=\"Represent the query for retrieval: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = PyPDFDirectoryLoader('./data')\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import  PromptTemplate\n",
    "\n",
    "from typing import Literal\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Output if question is standalone or hydechain\"\"\"\n",
    "\n",
    "    datasource: Literal[\"hydechain\", \"standalone\"] = Field(\n",
    "        ...,\n",
    "        description=\"Determine whether to enter the process for handling declarative and normal sentences or interrogative and question sentences (indirect or direct questions) based on the user's query tone\",\n",
    "    )\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=RouteQuery)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\n",
    "    \"\"\"\n",
    "        As an AI assistant for application route, determine whether to enter the process for handling declarative and normal sentences or interrogative and question sentences (indirect or direct questions) based on the user's query tone inside [Query] marks,\n",
    "        If it is a declarative and normal sentence process, return the string: \"hydechain\"; \n",
    "        otherwise, return: \"standardalonequery\".\n",
    "\n",
    "        Notice: \n",
    "        Do not answer the query or make up the query, only return as simple as possible, eithter \"standardalonequery\" or \"hydechain\" as string without any instruction text, reasoning text, headlines, leading-text or other additional information.\n",
    "        \n",
    "        \\n{format_instructions}\\n{query}\\n\n",
    "        \n",
    "        \"\"\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "\n",
    "route_chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Generate a different version of the text inside [Origin text] marks with the help of conversation history to retrieve relevant documents from a vector storage.\n",
    "The version is for better model comprehension while maintaining the original text sentiment and brevity.\n",
    "Your goal is to help the user overcome some of the limitations of the distance-based similarity search. \n",
    "Notice: Only return the reformulated statement without any instruction text, reasoning text, headlines, leading-text or other additional information.\"\"\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"[Origin text]\\n{query}\\n[Origin text]\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "hyde_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"\"\"Given a conversation history and a follow-up query inside [Query] marks, rephrase the follow-up query to be a standalone query. \\\n",
    "Do NOT answer the query, just reformulate it if needed, otherwise return it as is.\n",
    "Notice: Only return the final standalone query as simple as possible without any instruction text, headlines, leading-text or other additional information.\"\"\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"[Query]{query}[Query]\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "standalone_chain =  prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(info):\n",
    "    if \"standalone\" in info[\"next_step\"].datasource:\n",
    "        return standalone_chain\n",
    "\n",
    "    if \"hydechain\" in info[\"next_step\"].datasource:\n",
    "        return hyde_chain\n",
    "\n",
    "    raise ValueError(\"Invalid next step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Answer query inside [Query] marks solely based on the following context:\\n{context}\\n\"\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"history\"),\n",
    "            HumanMessagePromptTemplate.from_template(\"[Query]{query}[Query]\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "temp = []\n",
    "def return_source(x):\n",
    "    temp.append([x])\n",
    "    return x\n",
    "\n",
    "\n",
    "mid_chain = (\n",
    "        RunnablePassthrough.assign(next_step=route_chain)\n",
    "        | RunnablePassthrough.assign(context=(RunnableLambda(choose_route) | retriever))\n",
    "        | RunnableLambda(return_source)\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableFieldSpec\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"In memory implementation of chat message history.\"\"\"\n",
    "\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        \"\"\"Add a self-created message to the store\"\"\"\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(user_id: str, conversation_id: str) -> BaseChatMessageHistory:\n",
    "    if (user_id, conversation_id) not in store:\n",
    "        store[(user_id, conversation_id)] = InMemoryHistory()\n",
    "    return store[(user_id, conversation_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = RunnableWithMessageHistory(\n",
    "    mid_chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"user_id\",\n",
    "            annotation=str,\n",
    "            name=\"User ID\",\n",
    "            description=\"Unique identifier for the user.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"conversation_id\",\n",
    "            annotation=str,\n",
    "            name=\"Conversation ID\",\n",
    "            description=\"Unique identifier for the conversation.\",\n",
    "            default=\"\",\n",
    "            is_shared=True,\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run fc8f6393-27af-4013-9550-ba90ff84a717 not found for run 3ba02814-b75d-4429-b776-ac08f67e1791. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is astute ai?', 'history': [], 'next_step': RouteQuery(datasource='standalone'), 'context': [Document(page_content='employees, send them dynamic messages via Slack, and help them take \\naction in real time. “You may fail but you must come back stronger!”  \\nResearch indicates that  62 to 73 percent of AI projects fail \\nonly when the guidance to integrate Artificial Intelligence \\nwith your business is shattered but guess what? we excel \\nat it. Astute integrates AI with your business like no one \\nelse can level it u p. But you can Smile; Astute is here!!! we \\ntransform the pitfalls into blossoms! Collaborating with \\nAstuteAi helps you address your problems in handling AI \\nand chatbots. Astute enhances the Business process with \\ninnovative generative AI Solutions. We trans form business \\nworkflows and performances by integrating Artificial \\nIntelligence.   \\n \\n \\nAstuteAI is a platform where innovation meets efficiency; \\nit’s not just a tool but a real Game -changer. And yes we \\nmade it !!! Astute won the Most prestigious Samsung \\nprism Hackathon! Don’t hold on - Collab with us now and', metadata={'page': 4, 'source': 'data\\\\(1) The ROI of AI - ASTUTE.pdf'}), Document(page_content=\"AI is not the future but a present -day reality transforming \\nbusinesses across various fields of work!  \\nArtificial intelligence seems to be complicated, but with the help of \\nAstute, it’s not!  \\nWe dive deep into the ocean of AI and bring out the best to be \\nproduced.  \\nAI can be a boon to your business but gets \\ncomplicated only when it's not organised properly. \\nIt can be a Real revenue driver when used in the \\nmost effective Way. Astute can make it happen for \\nyou - We will eliminate your fear of  using AI \\nsolutions in you r business and transform your \\ndreams into Reality. Feel happy, you found us to \\nboost your business with AI!     \\nAI-based works mostly report 20% reduction in \\noperational costs than the usual human -based \\nmanual works.  And this is the time where the question \\nregarding integrating AI with our business has \\ntransformed from “whether to implement AI in our \\nbusinesses or not” to “how to measure its return on\", metadata={'page': 1, 'source': 'data\\\\(1) The ROI of AI - ASTUTE.pdf'}), Document(page_content=\"the location.  \\n• Streamlined Collaboration:  SaaS fosters seamless \\ncollaboration. Multiple users can work within the same \\napplication simultaneously, ensuring everyone stays on \\nthe same page. Boost teamwork and project efficiency \\nwith the power of SaaS.  \\nAstute AI: The Future of Business Management is Here  \\nWe've talked about the power of AI and the convenience of \\nSaaS, but what if you could have both in one revolutionary \\npackage? That's where Astute AI comes in!  \\nImagine AI -powered business management tools, delivered \\nwith the ease and affordability of SaaS. Astute AI removes the \\ncomplexity, offering a streamlined service that takes your \\nbusiness to the next level.  \\nHere's how Astute AI empowers you:  \\n• Unleash the Power of AI:  Stop relying on outdated \\nmethods. Astute AI integrates cutting -edge AI directly into\", metadata={'page': 1, 'source': 'data\\\\Astute SaaS and AI.pdf'}), Document(page_content=\"king, but without the right tools, it's a burden, not a weapon.  \\n \\nIntroducing Astute: Your AI Business Powerhouse  \\nAstute is more than just software; it's your one -stop shop for a \\nsmarter, more efficient business strategy. We combine the power of \\nMachine Learning (ML) with a user -friendly, cloud -based SaaS \\nplatform to empower you to:  \\n \\nUnleash the Potential of Your Data:   Ditch the spreadsheets and \\nmanual analysis. Astute's ML -powered data analytics tools unearth \\nvaluable insights from your data, giving you a clear picture of your \\ntarget audience, campaign performance, and market trends.  \\nEffortless AI Integration: We integrate cutting -edge AI directly into \\nour user -friendly, cloud -based SaaS platform. Gain valuable insights, \\nautomate tasks, and optimize processes – all within a single \\nplatform.  \\nA Perfect Fit for Any Business :  Astute's SaaS model makes it \\naccessible and scalable for businesses of all sizes. No hefty upfront\", metadata={'page': 2, 'source': 'data\\\\Astute Machine Learning.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "result = final_chain.invoke(\n",
    "        {\"query\": \"What is astute ai?\"},\n",
    "        config={\n",
    "            \"configurable\": {\"user_id\": \"user_id\", \"conversation_id\": \"conversation_id\"}\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Astute AI is a platform that combines Machine Learning (ML) with a user-friendly, cloud-based SaaS platform to empower businesses to unleash the potential of their data, effortlessly integrate AI, and optimize processes within a single platform. It is accessible and scalable for businesses of all sizes.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data\\\\(1) The ROI of AI - ASTUTE.pdf',\n",
       " 'data\\\\Astute Machine Learning.pdf',\n",
       " 'data\\\\Astute SaaS and AI.pdf'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x.metadata['source'] for x in temp[0][0]['context']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is astute ai?'),\n",
       " AIMessage(content=' Astute AI is a platform that combines Machine Learning (ML) with a user-friendly, cloud-based SaaS platform to empower businesses to unleash the potential of their data, effortlessly integrate AI, and optimize processes within a single platform. It is accessible and scalable for businesses of all sizes.')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store[('user_id','conversation_id')].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
